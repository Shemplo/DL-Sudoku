# -*- coding: utf-8 -*-
"""Sudoku recognition.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EtgaSNXZifKQnojdv4OI6wU4mQMAHGZv
"""

import os
import re
import cv2
import math
import random
import skimage
import itertools
import numpy as np
import pandas as pd
import tensorflow as tf
import matplotlib.pyplot as plt

from skimage import draw, io
from functools import reduce
from IPython.display import clear_output
from PIL import Image, ImageDraw, ImageFont, ImageOps

def show_images (images):
  plt.figure (figsize = (7.5, 7.5))
  for index, (title, image) in enumerate(images.items()):
    plt.subplot(1, len(images), index + 1)
    plt.title(title)
    plt.imshow(tf.keras.preprocessing.image.array_to_img(image), cmap='gray')
    plt.axis('off')
  plt.show()

__block_img_pattern = "block-[0-8]\\.png"
__dir_name_pattern = ".*?/\d{4}-\d{2}-\d{2}T\d{2}.*"

__block_img_matcher = lambda value: 1 if (re.match (__block_img_pattern, value)) else 0
__solution_matcher = lambda value: 1 if (re.match ("solution\\.txt", value)) else 0
__matrix_matcher = lambda value: 1 if (re.match ("matrix\\.txt", value)) else 0

def __files_counter (dir_files, pattern):
    mask = map (pattern, dir_files)
    return reduce (lambda a, b: a + b, mask, 0)

def __is_template_dir (dir_name, dir_files):
    return re.match (__dir_name_pattern, dir_name) \
        and __files_counter (dir_files, __block_img_matcher) == 9 \
        and __files_counter (dir_files, __solution_matcher) == 1 \
        and __files_counter (dir_files, __matrix_matcher) == 1

def __prepare_templte (dir_name, files):
    blocks = list (filter (__block_img_matcher, files))
    return (dir_name, blocks, "matrix.txt", "solution.txt")

def find_templates (dir):
    return [__prepare_templte (dir_name, files) for dir_name, dirs, files in os.walk (dir) if __is_template_dir (dir_name, files)]

template_dirs = find_templates ("drive/My Drive/Colab Notebooks/sudokus")
print ("Template directories: " + str (len (template_dirs)))

def template2data (dir_name, blocks, matrix_file, solution_file):
    blocks = sorted (blocks)

    matrix = []
    with open (dir_name + "/" + matrix_file, "rt") as f:
        for line in f.readlines ():
            for v in re.split ("\\s+", line.strip ()):
              matrix.append (int (v))
    matrix = np.array (matrix).reshape (9, 9)

    images = []
    submatrices = []
    for i in range (9):
      row, col = i // 3, i % 3
      sub = matrix [row * 3 : row * 3 + 3, col * 3 : col * 3 + 3].reshape (9)
      
      images.append (dir_name + "/" + blocks [i])
      submatrices.append (sub)

    return images, submatrices

templates = list (map (lambda a: template2data (a [0], a [1], a [2], a [3]), template_dirs));
print ("Templates: " + str (len (templates)));

paths = [templates [i][0][j] for i in range (0, len (templates)) for j in range (0, len (templates [i][0]))]
matrices = [templates [i][1][j] for i in range (0, len (templates)) for j in range (0, len (templates [i][1]))]

print ("Dataset size: " + str (min (len (paths), len (matrices))));

IMAGE_SIZE = 64
IMAGE_CHANNELS = 3

@tf.function
def preprocess (path, matrix):
  image = tf.io.read_file (path)
  image = tf.io.decode_image (image, channels = IMAGE_CHANNELS, expand_animations = False, dtype = tf.uint8)

  image = tf.cast (tf.image.resize (image, size = (IMAGE_SIZE, IMAGE_SIZE)), tf.uint8)
  image = tf.image.convert_image_dtype (image, tf.float32)

  return image, matrix / 9

dataset = tf.data.Dataset.from_tensor_slices ((paths, matrices)).map (preprocess)
dataset

subset = list (dataset.take (9))
show_images ({'Part of sudoku 1': subset [0][0], 'Part of sudoku 2': subset [1][0], 'Part of sudoku 3': subset [2][0]})
show_images ({'Part of sudoku 4': subset [3][0], 'Part of sudoku 5': subset [4][0], 'Part of sudoku 6': subset [5][0]})
show_images ({'Part of sudoku 7': subset [6][0], 'Part of sudoku 8': subset [7][0], 'Part of sudoku 9': subset [8][0]})

print (subset [0][1])

TRAIN_SIZE = 10005
BATCH_SIZE = 100
BUFFER_SIZE = 200
STEPS_PER_EPOCH = TRAIN_SIZE // BATCH_SIZE

train_dataset = dataset.take (TRAIN_SIZE).cache ().shuffle (BUFFER_SIZE)\
              . batch (BATCH_SIZE).repeat ()\
              . prefetch (buffer_size = tf.data.experimental.AUTOTUNE)
test_dataset = dataset.skip (TRAIN_SIZE).batch (BATCH_SIZE)
print (f'Train dataset: {train_dataset}')
print (f'Test dataset: {test_dataset}')

print (train_dataset.take (1))
print (test_dataset.take (1))

#for features, label in dataset.take(1): 
#  print(features, label)

model = tf.keras.models.Sequential ([
  tf.keras.layers.Conv2D (3, kernel_size = 2, input_shape = (64, 64, 3)),
  tf.keras.layers.BatchNormalization (),
  tf.keras.layers.Conv2D (3, kernel_size = 2, input_shape = (48, 48, 3)),
  tf.keras.layers.BatchNormalization (),
  tf.keras.layers.Conv2D (2, kernel_size = 2, input_shape = (32, 32, 3)),
  tf.keras.layers.BatchNormalization (),
  tf.keras.layers.Flatten (),
  tf.keras.layers.Dense (32 * 32 * 2, activation = 'sigmoid'),
  tf.keras.layers.Dropout (0.1),
  tf.keras.layers.Dense (16 * 16 * 2, activation = 'sigmoid'),
  tf.keras.layers.Dropout (0.1),
  tf.keras.layers.Dense (16 * 16 * 1, activation = 'sigmoid'),
  tf.keras.layers.BatchNormalization (),
  tf.keras.layers.Dense (16 * 16 * 1, activation = 'relu'),
  tf.keras.layers.Dense (8 * 8 * 1, activation = 'relu'),
  tf.keras.layers.Dense (8 * 8 * 1, activation = 'relu'),
  tf.keras.layers.Dense (9, activation = 'sigmoid')
])

model.compile (optimizer = 'adam', loss = tf.keras.losses.BinaryCrossentropy (from_logits = False), metrics = ['accuracy'])

model.summary()
tf.keras.utils.plot_model(model, show_shapes=True)

class DisplayCallback (tf.keras.callbacks.Callback):
  def on_epoch_end (self, epoch, logs=None):
    clear_output (wait=True)
    #show_predictions ()
    print ('\nSample Prediction after epoch {}\n'.format (epoch + 1))

EPOCHS = 50
VAL_SUBSPLITS = 2
VALIDATION_STEPS = 3

test_size = min (len (paths), len (matrices)) - TRAIN_SIZE
print(f'Train size: {TRAIN_SIZE}, test size: {test_size}')
print(f'BATCH_SIZE = {BATCH_SIZE}, VALIDATION_STEPS = {VALIDATION_STEPS}, STEPS_PER_EPOCH = {STEPS_PER_EPOCH}')

#mh_v1 = model_history
model_history = model.fit (train_dataset, epochs = EPOCHS, steps_per_epoch = STEPS_PER_EPOCH,
  validation_steps = VALIDATION_STEPS, validation_data = test_dataset, callbacks = [])

loss = model_history.history['loss']
val_loss = model_history.history['val_loss']

epochs = range(EPOCHS)

plt.figure()
plt.plot(epochs, loss, 'r', label='Training loss')
plt.plot(epochs, val_loss, 'bo', label='Validation loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss Value')
plt.ylim([0.2, 0.85])
plt.legend()
plt.show()

accuracy = model_history.history['accuracy']
val_accuracy = model_history.history['val_accuracy']

epochs = range(EPOCHS)

plt.figure()
plt.plot(epochs, accuracy, 'r', label='Training accuracy')
plt.plot(epochs, val_accuracy, 'bo', label='Validation accuracy')
plt.title('Training and Validation accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy Value')
plt.ylim([0, 1])
plt.legend()
plt.show()

